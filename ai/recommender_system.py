# -*- coding: utf-8 -*-
"""recommender_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-JaRHWCYocpaBYY6eC6XoA4iu96xRlS-

<h1>GET DATA</h1>
<p style="color:red">get data from Data sources and create a file for this data</p>
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install ta vaderSentiment
import yfinance as yf
import pandas as pd
import numpy as np
from ta import add_all_ta_features  # Ø§Ú¯Ø± ÙÙ‚Ø· RSI Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒØ¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ta Ø±Ø§ Ø­Ø°Ù Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· RSI Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒØ¯
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import requests
from datetime import datetime, timedelta

# === ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ===
tickers = [
    "NVDA", "MSFT", "AAPL", "GOOGL", "GOOG", "AMZN", "META", "TSLA", "AVGO", "ASML",
    "NFLX", "AMD", "COST", "PEP", "LIN", "TMUS", "QCOM", "TXN", "AMAT", "INTU",
    "ADP", "REGN", "KLAC", "VRTX", "ISRG", "PANW", "SNPS", "CDNS", "MELI", "PYPL",
    "CTAS", "CSGP", "MRNA", "MAR", "CSX", "EA", "FAST", "CPRT", "EXC", "ROP",
    "PCAR", "VRSK", "DDOG", "FANG", "GILD", "GEHC", "IDXX", "ZS", "ODFL", "TTD",
    "MNST", "WDAY", "ABNB", "FTNT", "DASH", "DLTR", "PAYX", "AEP", "KDP", "CTSH",
    "BKNG", "ON", "CRWD", "MRVL", "NXPI", "LRCX", "MU", "ADI", "HON", "SBUX",
    "PDD", "CMG", "ORLY", "NXST", "VST", "TSCO", "ROST", "A", "TTWO", "BKR",
    "FICO", "YUM", "DLR", "HWM", "CCEP", "MSTR", "WELL", "XEL", "CLPBY",
    "EOG", "PSA", "URI"
]  # Ù„ÛŒØ³Øª ØªÛŒÚ©Ø±Ù‡Ø§

api_key = "fd21bf6f585f45beae7bafe36d1b04fe"  # Ú©Ù„ÛŒØ¯ NewsAPI Ø®ÙˆØ¯ØªÙˆÙ†

# === Û±. Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ (historical data Ø¨Ø§ Dividends Ùˆ Stock Splits) ===
print("ğŸ“¥ Downloading historical data...")
all_data = pd.DataFrame()
batch_size = 50  # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯ÛŒØª yfinance
for i in range(0, len(tickers), batch_size):
    batch = tickers[i:i + batch_size]
    try:
        data_batch = yf.download(
            batch,
            start="2020-01-01",
            end="2025-10-05",
            group_by="ticker",
            auto_adjust=False,
            actions=True
        )
        all_data = pd.concat([all_data, data_batch])
        print(f"Downloaded batch {i//batch_size + 1}")
    except Exception as e:
        print(f"Error in batch {i//batch_size + 1}: {e}")

# ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨
historical_data = all_data.stack(level=0, future_stack=True).reset_index().rename(columns={"level_1": "Ticker"})
historical_data["Date"] = pd.to_datetime(historical_data["Date"])
historical_data = historical_data.dropna(subset=["Open", "High", "Low", "Close"])
historical_data = historical_data.drop_duplicates(subset=["Date", "Ticker"])
historical_data = historical_data.sort_values(by=["Ticker", "Date"])

# === Û². Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… Ø´Ø±Ú©Øª Ùˆ fundamentals ===
print("ğŸ“Š Adding company info and fundamentals...")
company_info = []
for ticker in tickers:
    try:
        t = yf.Ticker(ticker)
        info = t.info
        company_name = info.get('longName', info.get('shortName', 'N/A'))
        company_info.append({
            "Ticker": ticker,
            "Company": company_name,
            "Market Cap": info.get("marketCap", None),
            "P/E Ratio": info.get("trailingPE", None),
            "EPS": info.get("trailingEps", None),
            "Sector": info.get("sector", "N/A"),
            "Industry": info.get("industry", "N/A")
        })
    except Exception as e:
        print(f"Error for {ticker}: {e}")
        company_info.append({
            "Ticker": ticker,
            "Company": "N/A",
            "Market Cap": None,
            "P/E Ratio": None,
            "EPS": None,
            "Sector": "N/A",
            "Industry": "N/A"
        })

company_df = pd.DataFrame(company_info)
enhanced_data = historical_data.merge(company_df, on="Ticker", how="left")

# === Û³. Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† technical indicators (Ø¨Ø§ ta - Ø§Ú¯Ø± Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù„Ø§Ø²Ù… Ù†ÛŒØ³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙÙ‚Ø· RSI Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯) ===
print("ğŸ“ˆ Adding technical indicators...")
enhanced_data = add_all_ta_features(enhanced_data, open="Open", high="High", low="Low", close="Close", volume="Volume")

# Ø§Ú¯Ø± ÙÙ‚Ø· RSI Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒØ¯ØŒ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ):
# def calculate_rsi(series, period=14):
#     delta = series.diff()
#     gain = np.where(delta > 0, delta, 0)
#     loss = np.where(delta < 0, -delta, 0)
#     avg_gain = pd.Series(gain).rolling(window=period).mean()
#     avg_loss = pd.Series(loss).rolling(window=period).mean()
#     rs = avg_gain / avg_loss
#     rsi = 100 - (100 / (1 + rs))
#     return rsi
# enhanced_data["momentum_rsi"] = enhanced_data.groupby("Ticker")["Adj Close"].transform(calculate_rsi)

# === Û´. Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ùˆ ØªØ­Ù„ÛŒÙ„ sentiment (ÙÙ‚Ø· Û³Û° Ø±ÙˆØ² Ø§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ) ===
print("ğŸ“¡ Collecting news and analyzing sentiment...")
end_date = datetime.today()
start_date = end_date - timedelta(days=30)
start_str = start_date.strftime("%Y-%m-%d")
end_str = end_date.strftime("%Y-%m-%d")

news_data = []
analyzer = SentimentIntensityAnalyzer()
for ticker in tickers:
    try:
        url = f"https://newsapi.org/v2/everything?q={ticker} stock&from={start_str}&to={end_str}&sortBy=relevancy&language=en&apiKey={api_key}"
        response = requests.get(url)
        data_json = response.json()
        articles = data_json.get("articles", [])

        for article in articles[:20]:  # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ Û²Û° Ø®Ø¨Ø± Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªÛŒÚ©Ø± (Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø­Ø¯ÙˆØ¯ÛŒØª API)
            text = (article.get("title", "") or "") + " " + (article.get("description", "") or "")
            sentiment_score = analyzer.polarity_scores(text)["compound"]
            news_data.append({
                "Date": pd.to_datetime(article["publishedAt"].split("T")[0]),
                "Ticker": ticker,
                "Sentiment": sentiment_score
            })
    except Exception as e:
        print(f"âš ï¸ Error for {ticker}: {e}")

if news_data:
    news_df = pd.DataFrame(news_data)
    sentiment_daily = news_df.groupby(["Date", "Ticker"])["Sentiment"].mean().reset_index()
    enhanced_data = enhanced_data.merge(sentiment_daily, on=["Date", "Ticker"], how="left")
    enhanced_data["Sentiment"] = enhanced_data["Sentiment"].fillna(0)
else:
    enhanced_data["Sentiment"] = 0

# === Ûµ. ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ ÙÙ‚Ø· ÛŒÚ© ÙØ§ÛŒÙ„ ===
enhanced_data = enhanced_data.sort_values(by=["Ticker", "Date"])
enhanced_data.to_csv("final_enhanced_stock_dataset.csv", index=False)
print("\nğŸ’¾ Final dataset saved to final_enhanced_stock_dataset.csv")
print(f"Columns: {enhanced_data.columns.tolist()}")
print("\nğŸ“Š Sample:")
print(enhanced_data.head(5))

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit

df = pd.read_csv("final_enhanced_stock_dataset.csv")
print(f"INFO : \n\n {df.info} \n\n\n\n DESCRIBE : \n\n {df.describe}")

"""<h1>DATA ANALYSIS</h1>
<p>Data cleaning</p>
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit

# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df = pd.read_csv('final_enhanced_stock_dataset.csv')
df['Date'] = pd.to_datetime(df['Date'])  # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ø¨Ù‡ ÙØ±Ù…Øª datetime

# 2. Ù¾Ø± Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡
df['Sentiment'] = df['Sentiment'].fillna(0)  # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Sentiment Ø¨Ø§ 0
numeric_cols = df.select_dtypes(include=[np.number]).columns  # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
df[numeric_cols] = df.groupby('Ticker')[numeric_cols].ffill()  # Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± Ù‚Ø¨Ù„ÛŒ Ù‡Ø± ØªÛŒÚ©Ø±
df = df.dropna(thresh=len(df.columns) - 10)  # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÛŒØ´ Ø§Ø² 10 Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡ Ø¯Ø§Ø±Ù†Ø¯

# 3. Ø§ÛŒØ¬Ø§Ø¯ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù (Ø¨Ø§Ø²Ø¯Ù‡ 7 Ø±ÙˆØ²Ù‡) Ø¨Ø§ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
return_7d = df.groupby('Ticker')['Adj Close'].shift(-7) / df['Adj Close'] - 1  # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ø¯Ù‡ 7 Ø±ÙˆØ²Ù‡
df = pd.concat([df, return_7d.rename('Return_7d')], axis=1)  # Ø§ÙØ²ÙˆØ¯Ù† Ø³ØªÙˆÙ† Ø¨Ø§ concat Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‡Ø´Ø¯Ø§Ø±

# 4. ØªØ¹Ø±ÛŒÙ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
features = df.select_dtypes(include=[np.number]).columns.drop('Return_7d', errors='ignore').tolist()  # Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ

# 5. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªÛŒÚ©Ø±
for ticker in df['Ticker'].unique():  # Ø­Ù„Ù‚Ù‡ Ø±ÙˆÛŒ Ù‡Ø± ØªÛŒÚ©Ø±
    mask = df['Ticker'] == ticker  # ÙÛŒÙ„ØªØ± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÛŒÚ©Ø±
    ticker_df = df.loc[mask, features].fillna(0)  # Ù¾Ø± Ú©Ø±Ø¯Ù† NaN Ø¨Ø§ 0 Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
    scaler = StandardScaler()  # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Ú©ÛŒÙ„Ø±
    scaled = scaler.fit_transform(ticker_df)  # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    df.loc[mask, features] = scaled  # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡

# 6. Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù‡Ø¯Ù ÛŒØ§ ÙˆÛŒÚ˜Ú¯ÛŒ
df = df.dropna(subset=features + ['Return_7d'])  # Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† ÙˆÛŒÚ˜Ú¯ÛŒ ÛŒØ§ Ù‡Ø¯Ù

# 7. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ train/test Ø¨Ø§ TimeSeriesSplit
X = df[features]  # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
y = df['Return_7d']  # Ù‡Ø¯Ù
tscv = TimeSeriesSplit(n_splits=5)  # ØªÙ‚Ø³ÛŒÙ… Ø²Ù…Ø§Ù†ÛŒ Ø¨Ù‡ 5 Ø¨Ø®Ø´
for train_idx, test_idx in tscv.split(X):  # Ø­Ù„Ù‚Ù‡ Ø±ÙˆÛŒ ØªÙ‚Ø³ÛŒÙ…â€ŒÙ‡Ø§
    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]  # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]  # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ Ù‡Ø¯Ù
    # Ø§ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯

# 8. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡
df.to_csv('preprocessed_stock_data.csv', index=False)  # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
print("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
print(df.head())  # Ù†Ù…Ø§ÛŒØ´ 5 Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„
print(df.isnull().sum())  # Ù†Ù…Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡

import matplotlib.pyplot as plt

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡
df = pd.read_csv('preprocessed_stock_data.csv')

# Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ
key_features = ['Return_7d', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Sentiment']

# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¹Ø¨Ù‡â€ŒØ§ÛŒ
plt.figure(figsize=(10, 6))
df[key_features].boxplot()
plt.title('Box Plot for Outlier Detection')
plt.xticks(rotation=45)
plt.ylabel('Normalized Values')
plt.tight_layout()
plt.show()

# Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ø¨Ø§ Ø±ÙˆØ´ IQR
outliers = {}
for feature in key_features:
    Q1 = df[feature].quantile(0.25)  # Ú†Ø§Ø±Ú© Ø§ÙˆÙ„
    Q3 = df[feature].quantile(0.75)  # Ú†Ø§Ø±Ú© Ø³ÙˆÙ…
    IQR = Q3 - Q1  # Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¨ÛŒÙ†â€ŒÚ†Ø§Ø±Ú©ÛŒ
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outlier_count = len(df[(df[feature] < lower_bound) | (df[feature] > upper_bound)])
    outliers[feature] = outlier_count
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ø¯Ø± {feature}: {outlier_count}")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df = pd.read_csv('preprocessed_stock_data.csv')

# Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
key_features = ['Return_7d', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Sentiment']

# Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ø¨Ø§ Ø±ÙˆØ´ IQR
for feature in key_features:
    Q1 = df[feature].quantile(0.25)  # Ú†Ø§Ø±Ú© Ø§ÙˆÙ„
    Q3 = df[feature].quantile(0.75)  # Ú†Ø§Ø±Ú© Ø³ÙˆÙ…
    IQR = Q3 - Q1  # Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¨ÛŒÙ†â€ŒÚ†Ø§Ø±Ú©ÛŒ
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[feature] = df[feature].clip(lower_bound, upper_bound)  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ Ù…Ø¬Ø§Ø²

# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡
df.to_csv('preprocessed_stock_data_no_outliers.csv', index=False)
print("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ø­Ø°Ù Ù†Ù‚Ø§Ø· Ù¾Ø±Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
print(df.head())
print(df.isnull().sum())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df = pd.read_csv('preprocessed_stock_data_no_outliers.csv')

# 1. Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙÛŒ
print("Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙÛŒ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:")
key_features = ['Return_7d', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Sentiment']
print(df[key_features].describe())

# 2. Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø¨Ø±Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
plt.figure(figsize=(12, 8))
for i, feature in enumerate(key_features, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# 3. Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
plt.figure(figsize=(10, 8))
correlation_matrix = df[key_features].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# 4. Ø¢Ø²Ù…ÙˆÙ† Ù†Ø±Ù…Ø§Ù„ Ø¨ÙˆØ¯Ù† (Shapiro-Wilk)
for feature in key_features:
    stat, p_value = shapiro(df[feature])
    print(f"Ø¢Ø²Ù…ÙˆÙ† Shapiro-Wilk Ø¨Ø±Ø§ÛŒ {feature}: Statistic={stat:.3f}, p-value={p_value:.3f}")
    if p_value > 0.05:
        print(f"   {feature} ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ø¯Ø§Ø±Ø¯ (p > 0.05)")
    else:
        print(f"   {feature} ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ù†Ø¯Ø§Ø±Ø¯ (p <= 0.05)")

from sklearn.preprocessing import PowerTransformer
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df = pd.read_csv('preprocessed_stock_data_no_outliers.csv')

# ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ PowerTransformer
features_to_transform = ['Return_7d', 'momentum_rsi', 'trend_macd', 'volatility_atr']
pt = PowerTransformer(method='yeo-johnson')
df[features_to_transform] = pt.fit_transform(df[features_to_transform])

# Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡
df.to_csv('preprocessed_stock_data_transformed.csv', index=False)
print("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")

# 1. Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙÛŒ
print("Ø¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙÛŒ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ:")
key_features = ['Return_7d', 'momentum_rsi', 'trend_macd', 'volatility_atr', 'Sentiment']
print(df[key_features].describe())

# 2. Ø±Ø³Ù… Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø¨Ø±Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
plt.figure(figsize=(12, 8))
for i, feature in enumerate(key_features, 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[feature], kde=True)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# 3. Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
plt.figure(figsize=(10, 8))
correlation_matrix = df[key_features].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# 4. Ø¢Ø²Ù…ÙˆÙ† Ù†Ø±Ù…Ø§Ù„ Ø¨ÙˆØ¯Ù† (Shapiro-Wilk)
for feature in key_features:
    stat, p_value = shapiro(df[feature])
    print(f"Ø¢Ø²Ù…ÙˆÙ† Shapiro-Wilk Ø¨Ø±Ø§ÛŒ {feature}: Statistic={stat:.3f}, p-value={p_value:.3f}")
    if p_value > 0.05:
        print(f"   {feature} ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ø¯Ø§Ø±Ø¯ (p > 0.05)")
    else:
        print(f"   {feature} ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ù†Ø¯Ø§Ø±Ø¯ (p <= 0.05)")

"""<h1>FEATURE ENGINEERING</h1>


"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_regression, SelectKBest
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
df = pd.read_csv('preprocessed_stock_data_transformed.csv', parse_dates=['Date'])
df = df.sort_values(['Ticker', 'Date']).reset_index(drop=True)

# Ø­ÙØ¸ Ú©Ù¾ÛŒ Ø§Ø² Ticker
ticker_column = df['Ticker'].copy()

# 2. Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
print("Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ:")
df.info()
print("\nØ¢Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙÛŒ:")
print(df.describe())
print("\nØªØ¹Ø¯Ø§Ø¯ NaN Ø¯Ø± Ù‡Ø± Ø³ØªÙˆÙ†:")
print(df.isnull().sum())
print("\nØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ØªÛŒÚ©Ø±:")
print(df['Ticker'].value_counts())
print("\nØ¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ Return_7d:")
df['Return_7d'].hist(bins=50)

# 3. Ø­Ø°Ù ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù…â€ŒØ§Ù‡Ù…ÛŒØª
df = df.drop(columns=['Sentiment', 'Company', 'Sector', 'Industry'])

# 4. Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
# 4.1. Ù„Ú¯â€ŒÙ‡Ø§ Ùˆ ØªØºÛŒÛŒØ±Ø§Øª
for lag in [1, 3, 5]:
    df[f'Adj_Close_Lag_{lag}'] = df.groupby('Ticker')['Adj Close'].shift(lag)
    df[f'Return_Lag_{lag}'] = df.groupby('Ticker')['Return_7d'].shift(lag)

# 4.2. Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªØ­Ø±Ú© Ùˆ Ú©Ø±Ø§Ø³â€ŒØ§ÙˆÙˆØ±Ù‡Ø§
windows = [5, 10, 20]
for window in windows:
    df[f'SMA_{window}'] = df.groupby('Ticker')['Adj Close'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)
    df[f'EMA_{window}'] = df.groupby('Ticker')['Adj Close'].ewm(span=window, adjust=False, min_periods=1).mean().reset_index(0, drop=True)
df['SMA_Crossover'] = np.where(df['SMA_5'] > df['SMA_20'], 1, 0)

# 4.3. Ù†Ø³Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ù„ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
df['EPS'] = df['EPS'].replace(0, np.nan).fillna(df.groupby('Ticker')['EPS'].transform('mean'))
df['Market Cap'] = df['Market Cap'].replace(0, np.nan).fillna(df.groupby('Ticker')['Market Cap'].transform('mean'))
df['PE_to_EPS'] = df['P/E Ratio'] / df['EPS']
df['Volume_to_MarketCap'] = df['Volume'] / df['Market Cap']
df['RSI_MACD_Ratio'] = df['momentum_rsi'] / (df['trend_macd'].replace(0, 1e-6) + 1e-6)

# 4.4. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ³Ø§Ù†Ø§Øª
df['Volatility_Rolling_Std'] = df.groupby('Ticker')['Adj Close'].rolling(window=10, min_periods=1).std().reset_index(0, drop=True)
df['Sharpe_Ratio'] = df['Return_7d'] / (df['Volatility_Rolling_Std'] + 1e-6)

# 4.5. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§ Ø¨Ø§Ø²Ø§Ø±
df['Market_Return'] = df.groupby('Date')['Adj Close'].transform('mean')
# Ù…Ø­Ø§Ø³Ø¨Ù‡ Beta Ø¨Ø§ Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
beta_values = []
for ticker in df['Ticker'].unique():
    mask = df['Ticker'] == ticker
    adj_close = df.loc[mask, 'Adj Close']
    market_return = df.loc[mask, 'Market_Return']
    if len(adj_close) > 1 and len(market_return) > 1:
        beta = np.corrcoef(adj_close, market_return)[0, 1]
    else:
        beta = 0
    beta_values.extend([beta] * len(adj_close))
df['Beta'] = beta_values

# 4.6. Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ÛŒ Ø¨Ø§ PCA
tech_features = [col for col in df.columns if 'momentum_' in col or 'trend_' in col or 'volatility_' in col]
scaler = StandardScaler()
pca = PCA(n_components=5)
df_tech_scaled = pd.DataFrame(scaler.fit_transform(df[tech_features]), columns=tech_features, index=df.index)
pca_features = pca.fit_transform(df_tech_scaled)
for i in range(pca_features.shape[1]):
    df[f'PCA_Tech_{i+1}'] = pca_features[:, i]

# 4.7. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
df['Day_of_Week'] = df['Date'].dt.dayofweek
df['Month'] = df['Date'].dt.month
df['Quarter'] = df['Date'].dt.quarter

# 5. Ù…Ø¯ÛŒØ±ÛŒØª NaN Ø¨Ø§ Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ
df = df.infer_objects(copy=False)
df = df.groupby('Ticker', group_keys=False).apply(
    lambda x: x.interpolate(method='linear', limit_direction='both').fillna(0),
    include_groups=False
).reset_index()  # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ticker Ø¨Ù‡ Ø³ØªÙˆÙ†

# Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø³ØªÙˆÙ† Ticker
df['Ticker'] = ticker_column

# 6. Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± ØºÛŒØ±Ù…Ø¬Ø§Ø²
df = df.replace([np.inf, -np.inf], np.nan).fillna(0)

# 7. Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ
features = df.select_dtypes(include=[np.number]).columns.drop('Return_7d')
if len(features) > 0 and len(df) > 0:
    X = df[features]
    y = df['Return_7d']
    selector = SelectKBest(mutual_info_regression, k=min(50, len(features)))
    X_selected = selector.fit_transform(X, y)
    selected_features = features[selector.get_support()].tolist()
else:
    print("Ø®Ø·Ø§: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒ Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªÙ†Ø¯.")
    selected_features = []

# 8. Ø°Ø®ÛŒØ±Ù‡ DataFrame Ù†Ù‡Ø§ÛŒÛŒ
df_final = df[selected_features + ['Ticker', 'Date', 'Return_7d']]
df_final.to_csv('engineered_stock_data.csv', index=False)

print("Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡:", selected_features)
print(df_final.head())
print(df_final.shape)

"""<h1>EDA</h1>

"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
df = pd.read_csv('engineered_stock_data.csv')

# 1. Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ
print("General Information:")
df.info()
print("\nDescriptive Statistics:")
print(df.describe())

# 2. Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡
print("\nMissing Values:")
print(df.isnull().sum())

# 3. ØªÙˆØ²ÛŒØ¹ Return_7d
plt.figure(figsize=(10, 6))
sns.histplot(df['Return_7d'], bins=50, kde=True)
plt.title('Distribution of Return_7d')
plt.xlabel('Return_7d')
plt.ylabel('Frequency')
plt.show()

# 4. Ø¨Ø§Ú©Ø³â€ŒÙ¾Ù„Ø§Øª Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[['Return_7d', 'Beta', 'Sharpe_Ratio']])
plt.title('Box Plot of Key Features')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

# 5. Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
numeric_df = df.select_dtypes(include=[np.number])
plt.figure(figsize=(12, 8))
correlation_matrix = numeric_df.corr()
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix')
plt.show()

# 6. Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Return_7d Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø§Ù‡
df['Month'] = pd.to_datetime(df['Date']).dt.month
plt.figure(figsize=(12, 6))
sns.barplot(x='Month', y='Return_7d', data=df, estimator=np.mean)
plt.title('Average Return_7d by Month')
plt.xlabel('Month')
plt.ylabel('Average Return_7d')
plt.show()

# 7. Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ Return_7d Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Beta
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Beta', y='Return_7d', data=df)
plt.title('Return_7d vs Beta')
plt.xlabel('Beta')
plt.ylabel('Return_7d')
plt.show()

# 8. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¨ÛŒØ´ØªØ± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_df)
scaled_df = pd.DataFrame(scaled_data, columns=numeric_df.columns, index=df.index)
print("\nScaled Data Statistics:")
print(scaled_df.describe())

"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Documentation - EDA Approach</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            background-color: #f4f4f4;
        }
        .text-box {
            border: 2px solid #333;
            padding: 15px;
            background-color: #fff;
            border-radius: 5px;
            max-width: 600px;
            margin: 0 auto;
        }
    </style>
</head>
<body>
    <div class="text-box">
        <h2>EDA and Feature Engineering Documentation</h2>
        <p>Due to initially dirty data (e.g., noise, missing values, inconsistencies), the initial exploratory data analysis (EDA) was combined with data cleaning to make the dataset usable. After performing feature engineering (e.g., adding lags, PCA, and new features), a second EDA was conducted to evaluate the impact of these changes and ensure data quality for modeling. This iterative approach was necessary given the data's complexity. The process is documented as of 07:39 PM BST on Saturday, October 11, 2025.</p>
    </div>
</body>
</html>

<h1>MODEL</h1>
"""

# ====================================================================================================
# ğŸ§© Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ LightGBM LambdaRank
# ----------------------------------------------------------------------------------------------------
# Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø±Ø¯Ù‡ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø³Ù¾Ø³ Ø¢Ù† Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
# Ø¨Ù‡ Ø³Ù‡ Ø¨Ø®Ø´ train / validation / test ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
# ====================================================================================================

import pandas as pd
import numpy as np
import os

# ----------------------------------------------------------------------------------------------------
# ğŸ“¥ 1. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡
# ----------------------------------------------------------------------------------------------------
DATA_PATH = "engineered_stock_data.csv"  # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ CSV
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"âŒ ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯: {DATA_PATH}")

df = pd.read_csv(DATA_PATH, parse_dates=["Date"])
print("âœ… Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯:", df.shape)

# ----------------------------------------------------------------------------------------------------
# âš™ï¸ 2. Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
# ----------------------------------------------------------------------------------------------------
# Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ
required_cols = ["Ticker", "Date", "Return_7d"]
for col in required_cols:
    if col not in df.columns:
        raise ValueError(f"âŒ Ø³ØªÙˆÙ† '{col}' Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª.")

# Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø± Ø®Ø§Ù„ÛŒ Ø¯Ø± Return_7d
df = df.dropna(subset=["Return_7d"]).reset_index(drop=True)

# Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
non_feature_cols = ["Ticker", "Date", "Return_7d", "index"]
feature_cols = [c for c in df.columns if c not in non_feature_cols and np.issubdtype(df[c].dtype, np.number)]

# Ù¾Ø± Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ø¨Ø§ Ø±ÙˆØ´ forward-fill Ùˆ Ø³Ù¾Ø³ ØµÙØ±
df[feature_cols] = df[feature_cols].fillna(method="ffill").fillna(0)

# ----------------------------------------------------------------------------------------------------
# ğŸ· 3. Ø³Ø§Ø®Øª Ø¨Ø±Ú†Ø³Ø¨ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (relevance)
# ----------------------------------------------------------------------------------------------------
df = df.sort_values(["Date", "Ticker"]).reset_index(drop=True)
q25, q50, q75 = df["Return_7d"].quantile([0.25, 0.5, 0.75]).values

def rel_discrete(x):
    if x <= q25: return 0
    elif x <= q50: return 1
    elif x <= q75: return 2
    else: return 3

df["rel"] = df["Return_7d"].apply(rel_discrete)

# ----------------------------------------------------------------------------------------------------
# â± 4. ØªÙ‚Ø³ÛŒÙ… Ø²Ù…Ø§Ù†ÛŒ (Train / Validation / Test)
# ----------------------------------------------------------------------------------------------------
dates = sorted(df["Date"].dt.date.unique())
n_dates = len(dates)

test_frac, val_frac = 0.15, 0.10
train_end = int(n_dates * (1 - test_frac - val_frac))
val_end = int(n_dates * (1 - test_frac))

train_dates = dates[:train_end]
val_dates = dates[train_end:val_end]
test_dates = dates[val_end:]

train = df[df["Date"].dt.date.isin(train_dates)].reset_index(drop=True)
val = df[df["Date"].dt.date.isin(val_dates)].reset_index(drop=True)
test = df[df["Date"].dt.date.isin(test_dates)].reset_index(drop=True)

print(f"ğŸ§® ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: Train={len(train_dates)} Ø±ÙˆØ² | Val={len(val_dates)} Ø±ÙˆØ² | Test={len(test_dates)} Ø±ÙˆØ²")

# ----------------------------------------------------------------------------------------------------
# ğŸ“¦ 5. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ø±Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ LightGBM Ranker
# ----------------------------------------------------------------------------------------------------
def build_lgb_arrays(df_split):
    X = df_split[feature_cols].values
    y = df_split["rel"].values.astype(int)
    groups = df_split.groupby(df_split["Date"].dt.date).size().to_numpy()
    return X, y, groups

X_train, y_train, g_train = build_lgb_arrays(train)
X_val, y_val, g_val = build_lgb_arrays(val)
X_test, y_test, g_test = build_lgb_arrays(test)

print("âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ LightGBM Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯.")
print(f"ğŸ“Š X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}")
print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§: {len(feature_cols)}")

# ====================================================================================================

# ====================================================================================================
# ğŸ¯ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ LightGBM LambdaRank Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… ØªÙˆØµÛŒÙ‡â€ŒÚ¯Ø± Ø³Ù‡Ø§Ù…
# ----------------------------------------------------------------------------------------------------
# Ù‡Ø¯Ù:
#   - Ù…Ø¯Ù„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø¬Ø°Ø§Ø¨ÛŒØª Ø³Ù‡Ø§Ù… Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ
#   - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… LambdaRank (Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ ØªÙˆØµÛŒÙ‡â€ŒÚ¯Ø± Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)
# ====================================================================================================

import lightgbm as lgb
from lightgbm import early_stopping, log_evaluation
from sklearn.metrics import ndcg_score
import pandas as pd
import matplotlib.pyplot as plt

# ----------------------------------------------------------------------------------------------------
# âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„
# ----------------------------------------------------------------------------------------------------
ranker = lgb.LGBMRanker(
    objective="lambdarank",    # Ù†ÙˆØ¹ Ù‡Ø¯Ù Ù…Ø¯Ù„: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ØªØ¨Ù‡â€ŒØ§ÛŒ (Ranking)
    learning_rate=0.05,        # Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
    num_leaves=31,             # ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø±Ú¯â€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ø¯Ø±Ø®Øª
    min_data_in_leaf=20,       # Ø­Ø¯Ø§Ù‚Ù„ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± Ù‡Ø± Ø¨Ø±Ú¯
    n_estimators=2000,         # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ iteration
    random_state=42,           # Ù…Ù‚Ø¯Ø§Ø± Ø«Ø§Ø¨Øª Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±Ù¾Ø°ÛŒØ±ÛŒ
    metric="ndcg"              # Ù…Ø¹ÛŒØ§Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ: NDCG
)

# ----------------------------------------------------------------------------------------------------
# â¸ ØªØ¹Ø±ÛŒÙ callbackâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… Ùˆ Ú¯Ø²Ø§Ø±Ø´ Ù„Ø§Ú¯
# ----------------------------------------------------------------------------------------------------
callbacks = [
    early_stopping(stopping_rounds=100, verbose=True),  # ØªÙˆÙ‚Ù Ø²ÙˆØ¯Ù‡Ù†Ú¯Ø§Ù… Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø¨Ù‡Ø¨ÙˆØ¯
    log_evaluation(period=100)                          # Ú†Ø§Ù¾ ÙˆØ¶Ø¹ÛŒØª Ø¢Ù…ÙˆØ²Ø´ Ù‡Ø± 100 iteration
]

# ----------------------------------------------------------------------------------------------------
# ğŸš€ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
# ----------------------------------------------------------------------------------------------------
ranker.fit(
    X_train,                   # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
    y_train,                   # Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ§ÛŒ
    group=g_train.tolist(),    # Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÛŒ Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ticker)
    eval_set=[(X_val, y_val)], # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
    eval_group=[g_val.tolist()],
    callbacks=callbacks         # callbackÙ‡Ø§ Ø¨Ø±Ø§ÛŒ early stopping Ùˆ log
)

print("ğŸ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")



# Ø­Ù„ Ù…Ø´Ú©Ù„ CatBoost: Ø§Ú¯Ø± Ù…Ø¯Ù„ CatBoost Ø´Ù…Ø§ Ø¨Ù‡ Ù†Ø§Ù… 'catboost_ranker_optimized.cbm' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ
# Ø¢Ù† Ø±Ø§ Ø¨Ù‡ 'catboost_ranker_optimized.cbm' ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ø¯Ù‡ÛŒØ¯ ØªØ§ Ø¨Ø§ Ú©Ø¯ Walk-Forward Ø³Ø§Ø²Ú¯Ø§Ø± Ø´ÙˆØ¯ØŒ
# ÛŒØ§ Ù…ØªØºÛŒØ± CB_MODEL_PATH Ø±Ø§ Ø¯Ø± Ø³Ù„ÙˆÙ„ Walk-Forward Ø§ØµÙ„Ø§Ø­ Ú©Ù†ÛŒØ¯.
# ----------------------------------------------------------------------------------------------------
# ğŸ“ˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„
# ----------------------------------------------------------------------------------------------------
# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ØªØ¨Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª
# Ù†Ø§Ù… ÙØ§ÛŒÙ„ÛŒ Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„ Walk-Forward Ø¨Ù‡ Ø¢Ù† Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒØ¯: lgb_ranker_tuned.pkl
y_pred = ranker.predict(X_test)
import joblib

# Ù…ØªØºÛŒØ± Ù…Ø¯Ù„ Ø´Ù…Ø§ Ø¯Ø± Ø³Ù„ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´ LightGBM Ø¨Ù‡ Ù†Ø§Ù… "ranker" Ø¨ÙˆØ¯
LGB_MODEL_PATH = "lgb_ranker_tuned.pkl"

# Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„
joblib.dump(ranker, LGB_MODEL_PATH)
print(f"âœ… Ù…Ø¯Ù„ LightGBM Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {LGB_MODEL_PATH}")

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ (Ø¨Ø±Ø§ÛŒ ØªØ³Øª ÛŒØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡)
# lgb_ranker_loaded = joblib.load(LGB_MODEL_PATH)



# Ù…Ø­Ø§Ø³Ø¨Ù‡ NDCG Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª
ndcg = ndcg_score([y_test], [y_pred])
print(f"âœ… NDCG (Test Set): {ndcg:.4f}")

# Ø³Ø§Ø®Øª DataFrame Ø§Ø² Ù†ØªØ§ÛŒØ¬ ØªØ³Øª
test_results = pd.DataFrame(X_test, columns=feature_cols)
test_results["pred"] = y_pred
test_results["Ticker"] = test["Ticker"].values
test_results["Date"] = test["Date"].values
test_results["true_rel"] = y_test

# ----------------------------------------------------------------------------------------------------
# ğŸ’¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ù‡Ø§Ù… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø±ÙˆØ² ØªØ³Øª
# ----------------------------------------------------------------------------------------------------
last_date = test_results["Date"].max()
top_stocks = (
    test_results[test_results["Date"] == last_date]
    .sort_values("pred", ascending=False)
    .head(10)[["Ticker", "pred", "true_rel"]]
)

print("ğŸ“Š Recommended Stocks for the Last Test Day:")
display(top_stocks)

# ----------------------------------------------------------------------------------------------------
# ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø´Ú©Ù„ ÙÙˆÙ†Øª)
# ----------------------------------------------------------------------------------------------------
feature_importance = pd.DataFrame({
    "feature": feature_cols,
    "importance": ranker.booster_.feature_importance(importance_type="gain")
}).sort_values(by="importance", ascending=False)

plt.figure(figsize=(10, 8))
plt.barh(feature_importance["feature"][:20][::-1], feature_importance["importance"][:20][::-1])
plt.title("Top 20 Most Important Features", fontsize=14)
plt.xlabel("Feature Importance (Gain)", fontsize=12)
plt.ylabel("Feature", fontsize=12)
plt.tight_layout()
plt.show()

print("âœ… ØªØ­Ù„ÛŒÙ„ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ú©Ø§Ù…Ù„ Ø´Ø¯.")

# ====================================================================================================

# Commented out IPython magic to ensure Python compatibility.
# ===============================================================
# âœ… Optimized Cell: Train & Evaluate CatBoostRanker (Focus on Production Model)
# - Preprocessing, time-split, CatBoostRanker training (YetiRank)
# - Outputs: final metrics, top-N list, feature importances
# ===============================================================
# %pip install catboost
# ---------- imports ----------
import os
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")

# CatBoost
try:
    from catboost import CatBoostRanker, Pool
except Exception as e:
    raise ImportError("catboost is not installed. Install with: pip install catboost (or conda). Error: {}".format(e))

from sklearn.metrics import ndcg_score
from sklearn.preprocessing import StandardScaler

# ---------- SETTINGS ----------
DATA_PATH = "engineered_stock_data.csv"   # <-- change if needed
TOPK = 5      # compute NDCG@TOPK and Precision@TOPK
TOPN_OUT = 10 # final recommended top-N to save/show
RELEVANCE_THRESHOLD = 2 # rel >= 2 is considered relevant for Precision@K

# ---------- 1) Load data ----------
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"Data file not found: {DATA_PATH}")

df = pd.read_csv(DATA_PATH, parse_dates=["Date"])
print("Loaded:", df.shape)
# required columns check
for col in ["Ticker", "Date", "Return_7d"]:
    if col not in df.columns:
        raise ValueError(f"Required column missing: {col}")

# ---------- 2) Basic preprocessing & numeric feature selection ----------
df = df.dropna(subset=["Return_7d"]).reset_index(drop=True)
non_feature_cols = ["Ticker", "Date", "Return_7d", "index"]
feature_cols = [c for c in df.columns if c not in non_feature_cols and pd.api.types.is_numeric_dtype(df[c])]
print("Using numeric features:", len(feature_cols))

# fill NaNs simply
df[feature_cols] = df[feature_cols].fillna(method="ffill").fillna(0)

# standardize numeric features
scaler = StandardScaler()
df[feature_cols] = scaler.fit_transform(df[feature_cols])

# ---------- 3) Build discrete relevance labels (0..3) from Return_7d ----------
df = df.sort_values(["Date", "Ticker"]).reset_index(drop=True)
q25, q50, q75 = df["Return_7d"].quantile([0.25, 0.5, 0.75]).values
def rel_discrete(x):
    if x <= q25: return 0
    elif x <= q50: return 1
    elif x <= q75: return 2
    else: return 3
df["rel"] = df["Return_7d"].apply(rel_discrete)

# ---------- 4) Time-based split (train/val/test) ----------
unique_dates = sorted(df["Date"].dt.date.unique())
n_dates = len(unique_dates)
test_frac, val_frac = 0.15, 0.10
train_end = int(n_dates * (1 - test_frac - val_frac))
val_end = int(n_dates * (1 - test_frac))
train_dates = unique_dates[:train_end]
val_dates = unique_dates[train_end:val_end]
test_dates = unique_dates[val_end:]

train = df[df["Date"].dt.date.isin(train_dates)].reset_index(drop=True)
val = df[df["Date"].dt.date.isin(val_dates)].reset_index(drop=True)
test = df[df["Date"].dt.date.isin(test_dates)].reset_index(drop=True)
print(f"Dates split -> train: {len(train_dates)}, val: {len(val_dates)}, test: {len(test_dates)}")
print("Rows ->", train.shape, val.shape, test.shape)

# ---------- 5) Prepare CatBoost Pools (group_id per row) ----------
date_to_qid = {d:i for i,d in enumerate(sorted(df["Date"].dt.date.unique()))}
train_group_ids = train["Date"].dt.date.map(date_to_qid).values
val_group_ids = val["Date"].dt.date.map(date_to_qid).values
test_group_ids = test["Date"].dt.date.map(date_to_qid).values

train_pool = Pool(data=train[feature_cols], label=train["rel"], group_id=train_group_ids)
val_pool = Pool(data=val[feature_cols], label=val["rel"], group_id=val_group_ids)
test_pool = Pool(data=test[feature_cols], label=test["rel"], group_id=test_group_ids)

# Group sizes for metric calculation
g_test = test.groupby(test["Date"].dt.date).size().to_numpy()

# ---------- 6) Train CatBoostRanker (YetiRank - Robust Settings) ----------
print("\nPreparing CatBoost pools and training CatBoostRanker ...")
cb_params = {
    "iterations":3000, # Increased iterations
    "learning_rate":0.02, # Slightly reduced LR for better stability
    "depth":7, # Increased depth
    "loss_function":"YetiRank",
    "eval_metric":"NDCG:top={}".format(TOPK),
    "random_seed":42,
    "use_best_model":True,
    "early_stopping_rounds":150, # Increased stopping rounds
    "l2_leaf_reg": 3.0 # L2 regularization added for robustness
}
cb_ranker = CatBoostRanker(**{k:v for k,v in cb_params.items() if k!="verbose"})
print("Training CatBoostRanker (YetiRank) with enhanced settings...")
cb_ranker.fit(train_pool, eval_set=val_pool, verbose=100)

# ---------- 7) Evaluation utilities ----------
def per_query_metrics_from_preds(df_split, preds, groups, K=TOPK):
    # df_split must be ordered by Date/Ticker as earlier
    results = []
    idx = 0
    query_dates = df_split["Date"].dt.date.unique()
    for i, g in enumerate(groups):
        block_y = df_split["rel"].values[idx: idx+g]
        block_preds = preds[idx: idx+g]
        # NDCG
        try:
            ndcg = ndcg_score([block_y], [block_preds], k=K)
        except Exception:
            ndcg = np.nan
        # Precision@K: treat rel>=2 as relevant
        topk_idx = np.argsort(block_preds)[-K:][::-1] if g>0 else np.array([])
        prec = np.mean(block_y[topk_idx] >= RELEVANCE_THRESHOLD) if len(topk_idx)>0 else np.nan
        results.append({"date": query_dates[i], "ndcg": ndcg, f"prec@{K}": prec, "n_items": g})
        idx += g
    return results

def aggregate_metrics(results):
    ndcgs = [r["ndcg"] for r in results if not np.isnan(r["ndcg"])]
    precs = [r[f"prec@{TOPK}"] for r in results if not np.isnan(r[f"prec@{TOPK}"])]
    return {"ndcg_mean": np.nanmean(ndcgs) if len(ndcgs)>0 else np.nan, "prec_mean": np.nanmean(precs) if len(precs)>0 else np.nan}

# ---------- 8) Predictions & Evaluation on Test Set ----------
print("\nPredicting with CatBoost on test set...")
cb_preds = cb_ranker.predict(test_pool)
cb_results = per_query_metrics_from_preds(test, cb_preds, g_test, K=TOPK)
cb_metrics = aggregate_metrics(cb_results)
print("CatBoost Final Test Metrics -> NDCG@{} mean: {:.4f}, Precision@{} mean: {:.4f}".format(TOPK, cb_metrics["ndcg_mean"], TOPK, cb_metrics["prec_mean"]))

# ---------- 9) Top-N recommendations for last test day ----------
last_test_date = test["Date"].dt.date.unique()[-1]
print("\nLast test date:", last_test_date)

def topN_for_date(df_split, preds, date, N=TOPN_OUT):
    mask = df_split["Date"].dt.date == date
    if mask.sum() == 0:
        return pd.DataFrame()
    # Ensure correct indices are used for slicing preds
    indices = np.where(mask)[0]
    sub = df_split[mask].copy().reset_index(drop=True)
    sub["score"] = preds[indices]
    return sub.sort_values("score", ascending=False).head(N)[["Date","Ticker","score","Return_7d"]]

cb_topN = topN_for_date(test, cb_preds, last_test_date, N=TOPN_OUT)

print("\nTop-{} CatBoost recommendations for last test day:".format(TOPN_OUT))
display(cb_topN)

# save topN files
cb_topN.to_csv("topN_catboost_last_test_day.csv", index=False)
print("Saved topN csv.")

# ---------- 10) Feature importance plot (English titles) ----------
cb_imp = cb_ranker.get_feature_importance(type="PredictionValuesChange")
cb_fi = pd.DataFrame({"feature": feature_cols, "importance": cb_imp}).sort_values("importance", ascending=False).head(20)

plt.figure(figsize=(10,8))
plt.barh(cb_fi["feature"][::-1], cb_fi["importance"][::-1])
plt.title("CatBoost Top-20 Feature Importance (Optimized Model)", fontsize=14)
plt.xlabel("Feature Importance (PredictionValuesChange)", fontsize=12)
plt.ylabel("Feature", fontsize=12)
plt.tight_layout()
plt.show()

# ---------- 11) Save model ----------
cb_ranker.save_model("catboost_ranker_optimized.cbm")
print("Model saved: catboost_ranker_optimized.cbm")
# ===============================================================

# Commented out IPython magic to ensure Python compatibility.
# Install required packages if not already installed
# %pip install lightgbm

# ===============================================================
# âœ… Walk-forward evaluation for pre-trained LightGBM and CatBoostRanker
# - Uses saved models from previous runs (lgb_ranker_tuned.pkl, catboost_ranker_optimized.cbm)
# - Time-based walk-forward validation to assess model stability
# - Outputs: NDCG@K and Precision@K over multiple folds, recommendations
# ===============================================================
import os
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
from sklearn.metrics import ndcg_score
from catboost import Pool
import warnings
warnings.filterwarnings("ignore")

# ---------- SETTINGS ----------
DATA_PATH = "engineered_stock_data.csv"  # Must match previous runs
LGB_MODEL_PATH = "lgb_ranker_tuned.pkl"
CB_MODEL_PATH = "catboost_ranker_optimized.cbm"
TOPK = 5  # Compute NDCG@TOPK and Precision@K
TOPN_OUT = 10  # Top-N recommendations to show
N_FOLDS = 5  # Number of walk-forward folds

# ---------- 1) Load and preprocess data ----------
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"Data file not found: {DATA_PATH}")

df = pd.read_csv(DATA_PATH, parse_dates=["Date"])
df = df.dropna(subset=["Return_7d"]).reset_index(drop=True)
non_feature_cols = ["Ticker", "Date", "Return_7d", "index"]
feature_cols = [c for c in df.columns if c not in non_feature_cols and pd.api.types.is_numeric_dtype(df[c])]
df[feature_cols] = df[feature_cols].fillna(method="ffill").fillna(0)

# Scale features (must match preprocessing in Model 2)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[feature_cols] = scaler.fit_transform(df[feature_cols])

# Create relevance labels (same as Model 2)
df = df.sort_values(["Date", "Ticker"]).reset_index(drop=True)
q25, q50, q75 = df["Return_7d"].quantile([0.25, 0.5, 0.75]).values
def rel_discrete(x):
    if x <= q25: return 0
    elif x <= q50: return 1
    elif x <= q75: return 2
    else: return 3
df["rel"] = df["Return_7d"].apply(rel_discrete)

# ---------- 2) Walk-forward split ----------
unique_dates = sorted(df["Date"].dt.date.unique())
n_dates = len(unique_dates)
fold_size = n_dates // (N_FOLDS + 1)  # Reserve one fold for final test
folds = []
for i in range(N_FOLDS):
    train_end = (i + 1) * fold_size
    val_start = train_end
    val_end = train_end + fold_size
    train_dates = unique_dates[:train_end]
    val_dates = unique_dates[val_start:val_end]
    folds.append({
        "train": df[df["Date"].dt.date.isin(train_dates)].reset_index(drop=True),
        "val": df[df["Date"].dt.date.isin(val_dates)].reset_index(drop=True)
    })
test_dates = unique_dates[-fold_size:]
test = df[df["Date"].dt.date.isin(test_dates)].reset_index(drop=True)

# ---------- 3) Utility functions ----------
def build_lgb_arrays(df_split):
    X = df_split[feature_cols].values
    y = df_split["rel"].values.astype(int)
    groups = df_split.groupby(df_split["Date"].dt.date).size().to_numpy()
    return X, y, groups

def per_query_metrics(df_split, preds, groups, K=TOPK):
    results = []
    idx = 0
    query_dates = df_split["Date"].dt.date.unique()
    for i, g in enumerate(groups):
        block_y = df_split["rel"].values[idx: idx+g]
        block_preds = preds[idx: idx+g]
        try:
            ndcg = ndcg_score([block_y], [block_preds], k=K)
        except Exception:
            ndcg = np.nan
        topk_idx = np.argsort(block_preds)[-K:][::-1] if g>0 else np.array([])
        prec = np.mean(block_y[topk_idx] >= 2) if len(topk_idx)>0 else np.nan
        results.append({"date": query_dates[i], "ndcg": ndcg, f"prec@{K}": prec})
        idx += g
    return results

def aggregate_metrics(results):
    ndcgs = [r["ndcg"] for r in results if not np.isnan(r["ndcg"])]
    precs = [r[f"prec@{TOPK}"] for r in results if not np.isnan(r[f"prec@{TOPK}"])]
    return {"ndcg_mean": np.nanmean(ndcgs) if len(ndcgs)>0 else np.nan, "prec_mean": np.nanmean(precs) if len(precs)>0 else np.nan}

# ---------- 4) Load pre-trained models ----------
if not os.path.exists(LGB_MODEL_PATH) or not os.path.exists(CB_MODEL_PATH):
    raise FileNotFoundError("Model files not found. Ensure lgb_ranker_tuned.pkl and catboost_ranker_optimized.cbm are saved from previous runs.")

lgb_ranker = joblib.load(LGB_MODEL_PATH)
from catboost import CatBoostRanker
cb_ranker = CatBoostRanker()
cb_ranker.load_model(CB_MODEL_PATH)

# ---------- 5) Walk-forward evaluation ----------
lgb_results = []
cb_results = []
for i, fold in enumerate(folds):
    print(f"\nFold {i+1}/{N_FOLDS}")
    val = fold["val"]
    X_val, y_val, g_val = build_lgb_arrays(val)
    val_group_ids = val["Date"].dt.date.map({d:j for j,d in enumerate(sorted(val["Date"].dt.date.unique()))}).values

    # Evaluate LightGBM
    lgb_preds = lgb_ranker.predict(X_val)
    lgb_fold_results = per_query_metrics(val, lgb_preds, g_val)
    lgb_results.append(aggregate_metrics(lgb_fold_results))

    # Evaluate CatBoost
    val_pool = Pool(data=val[feature_cols], label=val["rel"], group_id=val_group_ids)
    cb_preds = cb_ranker.predict(val_pool)
    cb_fold_results = per_query_metrics(val, cb_preds, g_val)
    cb_results.append(aggregate_metrics(cb_fold_results))

# Evaluate on test set
X_test, y_test, g_test = build_lgb_arrays(test)
test_group_ids = test["Date"].dt.date.map({d:j for j,d in enumerate(sorted(test["Date"].dt.date.unique()))}).values
lgb_preds = lgb_ranker.predict(X_test)
lgb_test_results = per_query_metrics(test, lgb_preds, g_test)
lgb_test_metrics = aggregate_metrics(lgb_test_results)
test_pool = Pool(data=test[feature_cols], label=test["rel"], group_id=test_group_ids)
cb_preds = cb_ranker.predict(test_pool)
cb_test_results = per_query_metrics(test, cb_preds, g_test)
cb_test_metrics = aggregate_metrics(cb_test_results)

# ---------- 6) Aggregate and compare results ----------
lgb_ndcg_mean = np.mean([r["ndcg_mean"] for r in lgb_results])
lgb_prec_mean = np.mean([r["prec_mean"] for r in lgb_results])
cb_ndcg_mean = np.mean([r["ndcg_mean"] for r in cb_results])
cb_prec_mean = np.mean([r["prec_mean"] for r in cb_results])

comp_df = pd.DataFrame({
    "Model": ["LightGBM", "CatBoost"],
    f"NDCG@{TOPK} (Val Mean)": [lgb_ndcg_mean, cb_ndcg_mean],
    f"Precision@{TOPK} (Val Mean)": [lgb_prec_mean, cb_prec_mean],
    f"NDCG@{TOPK} (Test)": [lgb_test_metrics["ndcg_mean"], cb_test_metrics["ndcg_mean"]],
    f"Precision@{TOPK} (Test)": [lgb_test_metrics["prec_mean"], cb_test_metrics["prec_mean"]]
})
print("\nWalk-forward Evaluation Results:")
display(comp_df)

# ---------- 7) Top-N recommendations for last test day ----------
last_test_date = test["Date"].dt.date.unique()[-1]
print("\nLast test date:", last_test_date)

def topN_for_date(df_split, preds, date, N=TOPN_OUT):
    mask = df_split["Date"].dt.date == date
    if mask.sum() == 0:
        return pd.DataFrame()
    sub = df_split[mask].copy().reset_index(drop=True)
    sub["score"] = preds[mask.values.nonzero()[0]] if len(preds)==len(df_split) else preds[:mask.sum()]
    return sub.sort_values("score", ascending=False).head(N)[["Date", "Ticker", "score", "Return_7d"]]

lgb_topN = topN_for_date(test, lgb_preds, last_test_date)
cb_topN = topN_for_date(test, cb_preds, last_test_date)
print("\nTop-{} LightGBM recommendations for last test day:".format(TOPN_OUT))
display(lgb_topN)
print("\nTop-{} CatBoost recommendations for last test day:".format(TOPN_OUT))
display(cb_topN)

# Save recommendations
lgb_topN.to_csv("topN_lgbm_walkforward.csv", index=False)
cb_topN.to_csv("topN_catboost_walkforward.csv", index=False)
print("Saved topN csvs.")

# ---------- 8) Plot results ----------
plt.figure(figsize=(10, 6))
plt.plot([i+1 for i in range(N_FOLDS)], [r["ndcg_mean"] for r in lgb_results], label="LightGBM NDCG@5", marker="o")
plt.plot([i+1 for i in range(N_FOLDS)], [r["ndcg_mean"] for r in cb_results], label="CatBoost NDCG@5", marker="s")
plt.title("NDCG@5 Across Walk-forward Folds")
plt.xlabel("Fold")
plt.ylabel("NDCG@5")
plt.legend()
plt.grid()
plt.show()

# ---------- 9) Next steps recommendation ----------
print("\nNext Steps:")
if cb_ndcg_mean > lgb_ndcg_mean and cb_test_metrics["ndcg_mean"] > lgb_test_metrics["ndcg_mean"]:
    print("- CatBoost consistently outperforms LightGBM. Deploy CatBoost as the primary model.")
elif lgb_ndcg_mean > cb_ndcg_mean and lgb_test_metrics["ndcg_mean"] > cb_test_metrics["ndcg_mean"]:
    print("- LightGBM consistently outperforms CatBoost. Deploy LightGBM as the primary model.")
else:
    print("- Performance is close. Consider ensemble of LightGBM and CatBoost or further tuning.")
print("- Simulate transaction costs to ensure real-world applicability.")
print("- If results are stable, deploy the chosen model. If not, explore Transformer-based models.")

import pandas as pd
import numpy as np
from catboost import CatBoostRanker, Pool
import joblib
import matplotlib.pyplot as plt

# ---------- SETTINGS ----------
DATA_PATH = "engineered_stock_data.csv"
CB_MODEL_PATH = "catboost_ranker_optimized.cbm"
FEE_RATES = [0.001, 0.005, 0.01]  # Ú©Ø§Ø±Ù…Ø²Ø¯Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù (0.1%, 0.5%, 1%)
SLIPPAGE_RATE = 0.0005  # Ù„ØºØ²Ø´ Ù‚ÛŒÙ…Øª (0.05%)
TOPK = 5  # ØªØ¹Ø¯Ø§Ø¯ Ø³Ù‡Ø§Ù… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
SIM_DAYS = 10  # ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
INITIAL_CAPITAL = 10000  # Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø§ÙˆÙ„ÛŒÙ‡ (Ø¨Ù‡ Ø¯Ù„Ø§Ø±)

# ---------- 1) Load data and model ----------
df = pd.read_csv(DATA_PATH, parse_dates=["Date"])
cb_ranker = CatBoostRanker()
cb_ranker.load_model(CB_MODEL_PATH)

# Preprocess
df = df.dropna(subset=["Return_7d"]).reset_index(drop=True)
non_feature_cols = ["Ticker", "Date", "Return_7d", "index"]
feature_cols = [c for c in df.columns if c not in non_feature_cols and pd.api.types.is_numeric_dtype(df[c])]
df[feature_cols] = df[feature_cols].fillna(method="ffill").fillna(0)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[feature_cols] = scaler.fit_transform(df[feature_cols])
q25, q50, q75 = df["Return_7d"].quantile([0.25, 0.5, 0.75]).values
def rel_discrete(x): return 0 if x <= q25 else 1 if x <= q50 else 2 if x <= q75 else 3
df["rel"] = df["Return_7d"].apply(rel_discrete)

df = df.sort_values(["Date", "Ticker"]).reset_index(drop=True)
unique_dates = sorted(df["Date"].dt.date.unique())
test_dates = unique_dates[-SIM_DAYS:]

# ---------- 2) Simulate trading for different fee rates ----------
results = {rate: [] for rate in FEE_RATES}
portfolio = {rate: [] for rate in FEE_RATES}

for fee_rate in FEE_RATES:
    capital = INITIAL_CAPITAL
    for date in test_dates:
        daily_df = df[df["Date"].dt.date == date].reset_index(drop=True)
        X = daily_df[feature_cols].values
        preds = cb_ranker.predict(X)
        topk_idx = np.argsort(preds)[-TOPK:][::-1]
        top_stocks = daily_df.iloc[topk_idx]

        for _, stock in top_stocks.iterrows():
            return_7d = stock["Return_7d"]
            investment = capital / TOPK
            fee = investment * fee_rate
            slippage = investment * SLIPPAGE_RATE
            net_cost = fee + slippage
            net_return = (investment - net_cost) * (return_7d / 100)
            capital += net_return - net_cost
            portfolio[fee_rate].append({"Date": date, "Ticker": stock["Ticker"], "Return": return_7d, "Net_Return": net_return})

        results[fee_rate].append({"Date": date, "Capital": capital})

# ---------- 3) Aggregate results ----------
for fee_rate in FEE_RATES:
    results_df = pd.DataFrame(results[fee_rate])
    portfolio_df = pd.DataFrame(portfolio[fee_rate])
    total_return = (results_df["Capital"].iloc[-1] - INITIAL_CAPITAL) / INITIAL_CAPITAL
    daily_return = (results_df["Capital"].pct_change().mean()) * 100  # Ø¨Ø§Ø²Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯Ø±ØµØ¯
    print(f"\nFee Rate: {fee_rate*100:.1f}%")
    print(f"Total Return: {total_return:.2%}")
    print(f"Average Daily Return: {daily_return:.2f}%")

# ---------- 4) Plot capital over time for each fee rate ----------
plt.figure(figsize=(10, 6))
for fee_rate in FEE_RATES:
    results_df = pd.DataFrame(results[fee_rate])
    plt.plot(results_df["Date"], results_df["Capital"], marker="o", label=f"Fee {fee_rate*100:.1f}%")
plt.title("Capital Over Time with Different Transaction Fees")
plt.xlabel("Date")
plt.ylabel("Capital ($)")
plt.legend()
plt.grid()
plt.show()

# ---------- 5) Display top performing stocks for lowest fee rate ----------
print("\nTop 5 Performing Stocks (Fee 0.1%):")
display(pd.DataFrame(portfolio[FEE_RATES[0]]).sort_values("Net_Return", ascending=False).head(5))

import pandas as pd
import numpy as np
import joblib
from catboost import CatBoostRanker, Pool
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from datetime import timedelta # FIX: Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² timedelta

# ---------- SETTINGS (ØªÙ†Ø¸ÛŒÙ…Ø§Øª) ----------
DATA_PATH = "engineered_stock_data.csv"
CB_MODEL_PATH = "catboost_ranker_optimized.cbm"
INITIAL_CAPITAL = 100000 # Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø§ÙˆÙ„ÛŒÙ‡
TOPK = 5 # ØªØ¹Ø¯Ø§Ø¯ Ø³Ù‡Ø§Ù… Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¯Ø± Ù‡Ø± Ø±ÙˆØ²
FEE_RATE = 0.001 # Ú©Ø§Ø±Ù…Ø²Ø¯ (0.1%)
SLIPPAGE_RATE = 0.0005 # Ù„ØºØ²Ø´ Ù‚ÛŒÙ…Øª (0.05%)
TEST_SPLIT_RATIO = 0.2 # Ù†Ø³Ø¨Øª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª (Ø¨Ø±Ø§ÛŒ ØªØ¹ÛŒÛŒÙ† Ø´Ø±ÙˆØ¹ Ø¨Ú©â€ŒØªØ³Øª)

# ---------- Û±) Load Data and Preprocess (Ù‡Ù…Ø§Ù†Ù†Ø¯ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ) ----------
df = pd.read_csv(DATA_PATH, parse_dates=["Date"])
df = df.dropna(subset=["Return_7d"]).reset_index(drop=True)

# Define features and scale (Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¢Ù…ÙˆØ²Ø´ ØªØ·Ø§Ø¨Ù‚ Ú©Ø§Ù…Ù„ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯)
non_feature_cols = ["Ticker", "Date", "Return_7d", "index"]
feature_cols = [c for c in df.columns if c not in non_feature_cols and pd.api.types.is_numeric_dtype(df[c])]
df[feature_cols] = df[feature_cols].fillna(method="ffill").fillna(0)
scaler = StandardScaler()
df[feature_cols] = scaler.fit_transform(df[feature_cols])
df = df.sort_values(["Date", "Ticker"]).reset_index(drop=True)

# Define Test Set
unique_dates = sorted(df["Date"].dt.date.unique())
test_size = int(len(unique_dates) * TEST_SPLIT_RATIO)
test_dates = unique_dates[-test_size:]
test_df = df[df["Date"].dt.date.isin(test_dates)].reset_index(drop=True)

# ---------- Û²) Load Model ----------
cb_ranker = CatBoostRanker()
try:
    cb_ranker.load_model(CB_MODEL_PATH)
except Exception as e:
    print(f"Error loading model: {e}")
    raise FileNotFoundError("CatBoost model not found or corrupted.")

# ---------- Û³) Backtesting Simulation (Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ) ----------
capital_history = []
capital = INITIAL_CAPITAL

# Ù„ÛŒØ³Øª Ø³Ù‡Ø§Ù… Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ùˆ Ø²Ù…Ø§Ù† Ø®Ø±ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Û· Ø±ÙˆØ²Ù‡)
positions = {}

for date in test_dates:
    # Û±. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ù‡Ø§Ù… Ø¯Ø± Ø­Ø§Ù„ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ (Exit)
    closed_return = 0
    closed_positions = []

    # Ø¨Ø³ØªÙ† Ù¾ÙˆØ²ÛŒØ´Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Û· Ø±ÙˆØ² Ú©Ø§Ù…Ù„ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯
    for key, pos in positions.items():
        if (date - pos['Buy_Date']).days >= 7:
            return_7d_percent = pos['Return_7d']

            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ø¯Ù‡ Ù†Ø§Ø®Ø§Ù„Øµ (Ø³ÙˆØ¯/Ø²ÛŒØ§Ù†)
            gross_return = pos['Investment'] * (return_7d_percent / 100)

            # Ø§Ø¹Ù…Ø§Ù„ Ú©Ø§Ø±Ù…Ø²Ø¯ ÙØ±ÙˆØ´
            fee_sell = pos['Investment'] * FEE_RATE
            net_profit_loss = gross_return - fee_sell

            closed_return += net_profit_loss
            closed_positions.append(key)

            # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù†Ù‚Ø¯
            capital += pos['Investment']

    for key in closed_positions:
        del positions[key]

    # Û². Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø¨Ø§ Ø³ÙˆØ¯/Ø²ÛŒØ§Ù† Ø§Ù…Ø±ÙˆØ²
    capital += closed_return

    # Û³. Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ ÙˆØ±ÙˆØ¯ (Entry)
    daily_df = test_df[test_df["Date"].dt.date == date].reset_index(drop=True)

    if len(daily_df) < TOPK:
        capital_history.append({"Date": date, "Capital": capital})
        continue

    X = daily_df[feature_cols].values

    # FIX: ØªØ¨Ø¯ÛŒÙ„ group_ids Ø¨Ù‡ integer
    group_ids = np.zeros(len(daily_df), dtype=int)
    daily_pool = Pool(data=X, group_id=group_ids)
    preds = cb_ranker.predict(daily_pool)

    # Ø§Ù†ØªØ®Ø§Ø¨ TOPK Ø³Ù‡Ù… Ø¨Ø§ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²
    daily_df["Score"] = preds
    top_stocks = daily_df.sort_values("Score", ascending=False).head(TOPK)

    # Û´. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨Ø±Ø§ÛŒ ÙˆØ±ÙˆØ¯
    available_cash = capital # Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ø³ØªÙ† Ù¾ÙˆØ²ÛŒØ´Ù†â€ŒÙ‡Ø§ØŒ capital Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù†Ù‚Ø¯ Ø§Ø³Øª

    if available_cash <= 0:
        investment_per_stock = 0
    else:
        # ØªÙ‚Ø³ÛŒÙ… Ù…Ø³Ø§ÙˆÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡ Ù†Ù‚Ø¯ Ø¨Ø±Ø§ÛŒ TOPK Ø³Ù‡Ù…
        investment_per_stock = available_cash / TOPK

    # Ûµ. Ø§Ù†Ø¬Ø§Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø®Ø±ÛŒØ¯ Ùˆ Ú©Ø³Ø± Ø§Ø² capital
    for _, stock in top_stocks.iterrows():
        if investment_per_stock > 0:
            investment_amount = investment_per_stock

            # Ø§Ø¹Ù…Ø§Ù„ Ú©Ø§Ø±Ù…Ø²Ø¯ Ø®Ø±ÛŒØ¯ Ùˆ Ù„ØºØ²Ø´
            friction_cost = investment_amount * (FEE_RATE + SLIPPAGE_RATE)
            investment_net = investment_amount - friction_cost

            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÙˆØ²ÛŒØ´Ù† Ø¬Ø¯ÛŒØ¯
            positions[(stock["Ticker"], date)] = {
                "Buy_Date": date,
                "Investment": investment_net, # Ù…Ù‚Ø¯Ø§Ø± Ø®Ø§Ù„Øµ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡
                "Return_7d": stock["Return_7d"] # Ø¨Ø§Ø²Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Û· Ø±ÙˆØ²Ù‡ (Target)
            }

            # Ú©Ø³Ø± Ù‡Ø²ÛŒÙ†Ù‡ Ú©Ù„ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ (Investment_Amount) Ø§Ø² capital
            capital -= investment_amount

    # Û¶. Ø«Ø¨Øª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø§Ø±Ø²Ø´ Ù¾ÙˆØ±ØªÙÙˆÙ„ÛŒÙˆ
    # Ø§Ø±Ø²Ø´ Ù¾ÙˆØ±ØªÙÙˆÙ„ÛŒÙˆ = Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù†Ù‚Ø¯ (capital) + Ø§Ø±Ø²Ø´ Ù¾ÙˆØ²ÛŒØ´Ù† Ù‡Ø§ÛŒ Ø¨Ø§Ø² (Investment Net)
    portfolio_value = capital + sum(p['Investment'] for p in positions.values())
    capital_history.append({"Date": date, "Capital": portfolio_value})

# ---------- Û´) Risk Metrics (Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø±ÛŒØ³Ú©) ----------

history_df = pd.DataFrame(capital_history)
history_df["Date"] = pd.to_datetime(history_df["Date"])
history_df = history_df.drop_duplicates(subset=['Date'], keep='last')

# Ø§Ù„Ù) Sharpe Ratio (Ù†Ø±Ø® Ø´Ø§Ø±Ù¾)
RISK_FREE_RATE_DAILY = 0.04 / 252 # Ù†Ø±Ø® Ø³Ø§Ù„Ø§Ù†Ù‡ 4%
daily_returns_series = history_df["Capital"].pct_change().dropna()
excess_returns = daily_returns_series - RISK_FREE_RATE_DAILY
sharpe_ratio = np.sqrt(252) * (excess_returns.mean() / excess_returns.std())

# Ø¨) Max Drawdown (Ø­Ø¯Ø§Ú©Ø«Ø± Ø§ÙØª Ø³Ø±Ù…Ø§ÛŒÙ‡)
history_df["Cumulative_Max"] = history_df["Capital"].cummax()
history_df["Drawdown"] = history_df["Capital"] / history_df["Cumulative_Max"] - 1
max_drawdown = history_df["Drawdown"].min() * 100

# Ø¬) Ø¨Ø§Ø²Ø¯Ù‡ Ú©Ù„
total_return = (history_df["Capital"].iloc[-1] / INITIAL_CAPITAL - 1) * 100
history_df = history_df.set_index("Date")

# ---------- Ûµ) Display Results (Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬) ----------
print("="*50)
print(f"ğŸ’° Backtesting Results for CatBoost Ranker (TOP {TOPK})")
print("="*50)
print(f"Start Date: {test_dates[0]}")
print(f"End Date: {test_dates[-1]}")
print(f"Trading Days: {len(test_dates)}")
print(f"Total Return: {total_return:.2f}%")
print(f"Annualized Sharpe Ratio (Target: >1.0): {sharpe_ratio:.2f}")
print(f"Max Drawdown (MDD - Target: <10%): {max_drawdown:.2f}%")
print("="*50)

# Ù¾Ù„Ø§Øª Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±Ù…Ø§ÛŒÙ‡
plt.figure(figsize=(12, 6))
plt.plot(history_df.index, history_df["Capital"], label="CatBoost Strategy")
plt.plot(history_df.index, history_df["Cumulative_Max"], linestyle='--', color='gray', label="Peak Capital")
plt.title("Equity Curve: CatBoost Strategy vs Time")
plt.xlabel("Date")
plt.ylabel("Capital ($)")
plt.legend()
plt.grid(True)
plt.savefig("equity_curve.png")

# Ù¾Ù„Ø§Øª Drawdown
plt.figure(figsize=(12, 3))
plt.plot(history_df.index, history_df["Drawdown"] * 100, color='red')
plt.title("Drawdown Percentage Over Time")
plt.xlabel("Date")
plt.ylabel("Drawdown (%)")
plt.grid(True)
plt.savefig("drawdown_curve.png")

import pandas as pd
import numpy as np
import joblib
from catboost import CatBoostRanker, Pool
from sklearn.preprocessing import StandardScaler
from datetime import timedelta
import warnings
warnings.filterwarnings("ignore")

# =================================================================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª
# =================================================================
DATA_PATH = "engineered_stock_data.csv"
CB_MODEL_PATH = "catboost_ranker_optimized.cbm"
INITIAL_CAPITAL = 100000
TOPK = 5
TEST_SPLIT_RATIO = 0.2
RISK_FREE_RATE_DAILY = 0.04 / 252 # Ù†Ø±Ø® Ø¨Ø¯ÙˆÙ† Ø±ÛŒØ³Ú©

# Ø³Ù†Ø§Ø±ÛŒÙˆÙ‡Ø§ÛŒ ØªØ³Øª Ø­Ø³Ø§Ø³ÛŒØª
SENSITIVITY_TESTS = [
    # 1. ØªØ³Øª Ù†Ø±Ø® Ú©Ø§Ø±Ù…Ø²Ø¯
    {"name": "High Fee (0.5%)", "fee": 0.005, "days": 7, "volume_filter": None},
    {"name": "Very High Fee (1.0%)", "fee": 0.01, "days": 7, "volume_filter": None},
    # 2. ØªØ³Øª ÙÛŒÙ„ØªØ± Ù†Ù‚Ø¯ÛŒÙ†Ú¯ÛŒ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¬Ù… Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø§Ø²Ø§Ø±)
    {"name": "Liquidity Filter (Vol > 100K)", "fee": 0.001, "days": 7, "volume_filter": 100000},
    # 3. ØªØ³Øª Ø¯ÙˆØ±Ù‡ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ
    {"name": "Holding Period 5 Days", "fee": 0.001, "days": 5, "volume_filter": None},
    {"name": "Holding Period 10 Days", "fee": 0.001, "days": 10, "volume_filter": None},
]

SLIPPAGE_RATE = 0.0005 # Ø«Ø§Ø¨Øª Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† Ù„ØºØ²Ø´ Ù‚ÛŒÙ…Øª (0.05%)

# =================================================================
# ØªÙˆØ§Ø¨Ø¹ Utility (Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ú©â€ŒØªØ³Øª Ø¯Ø± Ù‡Ø± Ø³Ù†Ø§Ø±ÛŒÙˆ)
# =================================================================

def run_backtest(test_config, test_df, cb_ranker, feature_cols, all_df):
    """Ø§Ø¬Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ú©â€ŒØªØ³Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ù†Ø§Ø±ÛŒÙˆ"""

    fee_rate = test_config["fee"]
    holding_days = test_config["days"]
    volume_threshold = test_config["volume_filter"]

    capital = INITIAL_CAPITAL
    positions = {}
    capital_history = []

    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ØªØ¹Ø±ÛŒÙ Ø³ØªÙˆÙ† Ø¨Ø§Ø²Ø¯Ù‡ ØµØ­ÛŒØ­
    return_col = f'Return_{holding_days}d'
    if return_col not in all_df.columns:
         return_col = "Return_7d" # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø³ØªÙˆÙ† Ù¾ÛŒØ´ ÙØ±Ø¶

    for date in sorted(test_df["Date"].dt.date.unique()):

        # Û±. Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ù‡Ø§Ù… Ø¯Ø± Ø­Ø§Ù„ Ù†Ú¯Ù‡Ø¯Ø§Ø±ÛŒ (Exit)
        closed_return = 0
        closed_positions = []

        for key, pos in positions.items():
            if (date - pos['Buy_Date']).days >= holding_days:
                return_percent = pos[return_col]

                gross_return = pos['Investment'] * (return_percent / 100)
                fee_sell = pos['Investment'] * fee_rate
                net_profit_loss = gross_return - fee_sell

                closed_return += net_profit_loss
                closed_positions.append(key)
                capital += pos['Investment'] # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø³Ø±Ù…Ø§ÛŒÙ‡ Ø§ØµÙ„ÛŒ

        for key in closed_positions:
            del positions[key]

        capital += closed_return

        # Û². Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ùˆ ÙˆØ±ÙˆØ¯ (Entry)
        daily_df = test_df[test_df["Date"].dt.date == date].copy().reset_index(drop=True)

        # Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± Ù†Ù‚Ø¯ÛŒÙ†Ú¯ÛŒ (Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡)
        if volume_threshold is not None:
            try:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ (all_df) Ú©Ù‡ Ø´Ø§Ù…Ù„ Volume Ø§Ø³Øª (Ø§Ú¯Ø±Ú†Ù‡ Ù‡Ù†ÙˆØ² Scaled Ø§Ø³Øª)
                current_volume_info = all_df[all_df["Date"].dt.date == date][["Ticker", "Volume"]]
                daily_df = daily_df.merge(current_volume_info, on=["Ticker"], how="left")
                # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Volume (ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Volume Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© ÙˆÛŒÚ˜Ú¯ÛŒ ÙÙ†ÛŒ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ù†Ø´Ø¯Ù‡ØŒ Ø¯Ø± Ù…Ù‚ÛŒØ§Ø³ Ø§ØµÙ„ÛŒ Ø®ÙˆØ¯ Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯)
                daily_df = daily_df[daily_df["Volume"] > volume_threshold]
            except KeyError:
                if 'Volume' in test_config["name"]:
                    print(f"\n[WARNING] Column 'Volume' not found. Skipping Liquidity Filter for this scenario.")
                pass # Ø§Ú¯Ø± Ø³ØªÙˆÙ† Volume ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ ÙÛŒÙ„ØªØ± Ù†Ù‚Ø¯ÛŒÙ†Ú¯ÛŒ Ø§Ø¹Ù…Ø§Ù„ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.

        if len(daily_df) < TOPK:
            portfolio_value = capital + sum(p['Investment'] for p in positions.values())
            capital_history.append({"Date": date, "Capital": portfolio_value})
            continue

        X = daily_df[feature_cols].values
        group_ids = np.zeros(len(daily_df), dtype=int)
        daily_pool = Pool(data=X, group_id=group_ids)
        preds = cb_ranker.predict(daily_pool)

        daily_df["Score"] = preds
        top_stocks = daily_df.sort_values("Score", ascending=False).head(TOPK)

        available_cash = capital
        investment_per_stock = available_cash / TOPK if available_cash > 0 else 0

        # Û³. Ø§Ù†Ø¬Ø§Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø®Ø±ÛŒØ¯ Ùˆ Ú©Ø³Ø± Ø§Ø² capital
        for _, stock in top_stocks.iterrows():
            if investment_per_stock > 0:
                investment_amount = investment_per_stock
                friction_cost = investment_amount * (fee_rate + SLIPPAGE_RATE)
                investment_net = investment_amount - friction_cost

                pos_key = (stock["Ticker"], date)
                positions[pos_key] = {
                    "Buy_Date": date,
                    "Investment": investment_net,
                    return_col: stock["Return_7d"] # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Return_7d Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ®Ù…ÛŒÙ† Ø¨Ø§Ø²Ø¯Ù‡
                }
                capital -= investment_amount

        # Û´. Ø«Ø¨Øª ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø§Ø±Ø²Ø´ Ù¾ÙˆØ±ØªÙÙˆÙ„ÛŒÙˆ
        portfolio_value = capital + sum(p['Investment'] for p in positions.values())
        capital_history.append({"Date": date, "Capital": portfolio_value})

    # Ø¨Ø³ØªÙ† Ù¾ÙˆØ²ÛŒØ´Ù† Ù‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
    final_return = 0
    for _, pos in positions.items():
        return_percent = pos[return_col]
        gross_return = pos['Investment'] * (return_percent / 100)
        fee_sell = pos['Investment'] * fee_rate
        net_profit_loss = gross_return - fee_sell
        final_return += net_profit_loss

    capital += final_return

    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§
    history_df = pd.DataFrame(capital_history)
    history_df["Date"] = pd.to_datetime(history_df["Date"])
    history_df = history_df.drop_duplicates(subset=['Date'], keep='last').set_index("Date")

    total_return = (history_df["Capital"].iloc[-1] / INITIAL_CAPITAL - 1) * 100
    daily_returns_series = history_df["Capital"].pct_change().dropna()
    excess_returns = daily_returns_series - RISK_FREE_RATE_DAILY
    sharpe_ratio = np.sqrt(252) * (excess_returns.mean() / excess_returns.std())

    history_df["Cumulative_Max"] = history_df["Capital"].cummax()
    history_df["Drawdown"] = history_df["Capital"] / history_df["Cumulative_Max"] - 1
    max_drawdown = history_df["Drawdown"].min() * 100

    return {"Total Return": total_return, "Sharpe Ratio": sharpe_ratio, "Max Drawdown": max_drawdown}

# =================================================================
# Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ
# =================================================================

# Û±. Load Data and Preprocess (Ù‡Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„)
df = pd.read_csv(DATA_PATH, parse_dates=["Date"])
df = df.dropna(subset=["Return_7d"]).reset_index(drop=True)
non_feature_cols = ["Ticker", "Date", "Return_7d", "index"]
feature_cols = [c for c in df.columns if c not in non_feature_cols and pd.api.types.is_numeric_dtype(df[c])]

# Ø­ÙØ¸ ÛŒÚ© Ú©Ù¾ÛŒ Ø§Ø² Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ† Volume (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª) Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ù†Ù‚Ø¯ÛŒÙ†Ú¯ÛŒ
all_df = df.copy()

df[feature_cols] = df[feature_cols].fillna(method="ffill").fillna(0)
scaler = StandardScaler()
df[feature_cols] = scaler.fit_transform(df[feature_cols])
df = df.sort_values(["Date", "Ticker"]).reset_index(drop=True)

unique_dates = sorted(df["Date"].dt.date.unique())
test_size = int(len(unique_dates) * TEST_SPLIT_RATIO)
test_dates = unique_dates[-test_size:]
test_df = df[df["Date"].dt.date.isin(test_dates)].reset_index(drop=True)

# Û². Load Model
cb_ranker = CatBoostRanker()
cb_ranker.load_model(CB_MODEL_PATH)


# Û³. Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø­Ø³Ø§Ø³ÛŒØª
results_list = []
base_result = {"Total Return": 127.37, "Sharpe Ratio": 7.12, "Max Drawdown": -0.47}
results_list.append({"Scenario": "Base Case (0.1% Fee, 7 Days)", **base_result})

for test in SENSITIVITY_TESTS:
    print(f"Running scenario: {test['name']}...")
    # Ø§Ù†ØªÙ‚Ø§Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ØµÙ„ÛŒ (all_df) Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ÙÛŒÙ„ØªØ± Volume
    result = run_backtest(test, test_df, cb_ranker, feature_cols, all_df)
    results_list.append({"Scenario": test["name"], **result})

# Û´. Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
results_df = pd.DataFrame(results_list)

# ØªØºÛŒÛŒØ± ÙØ±Ù…Øª Ù†Ù…Ø§ÛŒØ´
results_df["Total Return"] = results_df["Total Return"].apply(lambda x: f"{x:.2f}%")
results_df["Sharpe Ratio"] = results_df["Sharpe Ratio"].apply(lambda x: f"{x:.2f}")
results_df["Max Drawdown"] = results_df["Max Drawdown"].apply(lambda x: f"{x:.2f}%")

print("\n" + "="*80)
print("             ğŸ“ˆ Final Robustness and Sensitivity Check Results ğŸ“ˆ")
print("="*80)
display(results_df)

print("\nInterpretation Notes:")
print("1. Compare the 'Sharpe Ratio' across different Fee Rates.")
print("2. Check if the 'Liquidity Filter' significantly degrades performance (Max Drawdown is key).")
print("3. See if the 'Holding Period' (5D/10D) maintains a high Sharpe Ratio.")

import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings("ignore")

# ---------- SETTINGS (Must match training environment) ----------
DATA_PATH = "engineered_stock_data.csv"
SCALER_PATH = "scaler.pkl"
TEST_VAL_FRAC = 0.25 # 15% Test + 10% Val = 25% Total for time split

# ---------- 1) Load data & Identify Features ----------
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"Data file not found: {DATA_PATH}")

df = pd.read_csv(DATA_PATH, parse_dates=["Date"])
df = df.dropna(subset=["Return_7d"]).reset_index(drop=True)

# Identify feature columns (same logic as in model training)
non_feature_cols = ["Ticker", "Date", "Return_7d", "index"]
feature_cols = [c for c in df.columns if c not in non_feature_cols and pd.api.types.is_numeric_dtype(df[c])]
print("Total Features found:", len(feature_cols))

# Handle NaNs (same logic as in model training)
df[feature_cols] = df[feature_cols].fillna(method="ffill").fillna(0)

# ---------- 2) Recreate Time-based Split to get TRAIN set ----------
df = df.sort_values(["Date", "Ticker"]).reset_index(drop=True)
unique_dates = sorted(df["Date"].dt.date.unique())
n_dates = len(unique_dates)

# Calculate the end index for the training set (1 - 0.25 = 0.75)
train_end_index = int(n_dates * (1 - TEST_VAL_FRAC))
train_dates = unique_dates[:train_end_index]

# Filter the original DataFrame to get the exact training data used for fitting
train = df[df["Date"].dt.date.isin(train_dates)].reset_index(drop=True)
print(f"Train data size for fitting scaler: {train.shape}")

# ---------- 3) Fit and Save StandardScaler on TRAIN data ONLY ----------
scaler = StandardScaler()

# Fit the scaler ONLY on the training features
scaler.fit(train[feature_cols])

# Save the fitted scaler object
joblib.dump(scaler, SCALER_PATH)

print("\nâœ… StandardScaler successfully fitted on training data and saved to:", SCALER_PATH)